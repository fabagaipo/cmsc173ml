# -*- coding: utf-8 -*-
"""Bagaipo_Project3_MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pPbBaPMsiANacH4qb_naOV7ufLXaU7v3

# Imports
"""

import pandas as pd
import numpy as np

"""# Data Reading and Cleaning"""

url = 'https://raw.githubusercontent.com/StatsGary/Data/main/thyroid_raw.csv'
df = pd.read_csv(url, header=None)
data = df.drop(df.iloc[:, 1:22], axis = 1)
data.columns = ["Feature1", "Feature2", "Feature3", "Feature4", "Feature5", "Target"]
data = data.drop(['Feature2', 'Feature4'], axis=1)
data.columns = ["Feature1", "Feature2", "Feature3", "Target"]
data = data.to_numpy()
data

data[:, -1]

my_map = {'negative': 0, 'sick': 1}
data[:, -1] = np.vectorize(my_map.get)(data[:, -1])
data

df = pd.DataFrame(data, columns = ["Feature1", "Feature2", "Feature3", "Target"])
df

"""# Splitting of Data"""

from sklearn import preprocessing

# Spliting data into Feature and
X=df.drop('Target', axis=1)
y=df['Target']
y=y.astype('int')

# Import train_test_split function
from sklearn.model_selection import train_test_split

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # 70% training and 30% test

"""# Training Data"""

# Import MLPClassifer
from sklearn.neural_network import MLPClassifier

# Create model object
clf = MLPClassifier(hidden_layer_sizes=(3,3),
                    max_iter=300,
                    random_state=5,
                    verbose=True,
                    activation='logistic',
                    learning_rate_init=0.1)

X_train, y_train

# Fit data onto the model
clf.fit(X_train,y_train)

# Make prediction on test dataset
y_pred=clf.predict(X_test)

"""# Model Evaluation"""

# Import scikit metrics and seaborn
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: %.3f%%" % (accuracy))

report = classification_report(y_test, y_pred)
print(report)

confusionMatrix = confusion_matrix(y_test, y_pred)
sns.heatmap(confusionMatrix, annot=True, fmt=".0f")

"""# Class MLP from Scratch"""

# Define the sigmoid activation function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Define the identity activation function
def identity(x):
    return x

# Define the MLP class
class MLP:
    def __init__(self, input_size, hidden_sizes, output_size):
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.output_size = output_size

        # Initialize weights for the hidden and output layers
        self.weights = []
        self.biases = []
        prev_size = input_size
        for i, size in enumerate(hidden_sizes):
            self.weights.append(np.random.randn(prev_size, size))
            self.biases.append(np.random.randn(size))
            prev_size = size
        self.weights.append(np.random.randn(prev_size, output_size))
        self.biases.append(np.random.randn(output_size))

    # Define the forward pass through the network
    def forward(self, input):
        activations = [input]
        for i, weight, bias in zip(range(len(self.weights)), self.weights, self.biases):
            # Use the sigmoid activation function for hidden layers
            if i < len(self.weights) - 1:
                activation = sigmoid(np.dot(activations[i], weight) + bias)
            # Use the identity activation function for the output layer
            else:
                activation = identity(np.dot(activations[i], weight) + bias)
            activations.append(activation)
        return activations

    # Define the backward pass through the network
    def backward(self, activations, targets):
        # Compute the error for the output layer
        output_error = targets - activations[-1]

        # Backpropagate the errors to compute the gradients
        gradients = []
        for i in reversed(range(len(self.weights))):
            activation = activations[i+1]
            if i == len(self.weights) - 1:
                # Use the identity derivative for the output layer
                gradient = output_error * 1
            else:
                # Use the sigmoid derivative for hidden layers
                gradient = np.dot(self.weights[i+1], output_error) * activation * (1 - activation)
            gradients.insert(0, gradient)

        # Compute the weight and bias updates
        weight_updates = []
        bias_updates = []
        for i, (weight, bias, activation, gradient) in enumerate(zip(self.weights, self.biases, activations, gradients)):
            weight_update = np.dot(activation.T, gradient)
            bias_update = gradient

"""To manually compute the forward and backward pass for an epoch in a multilayer perceptron (MLP), you can use the following steps:

- Define the input data and the corresponding targets for the epoch.
- Initialize the MLP with the appropriate input size, hidden layer sizes, and output size.
- Pass the input data through the MLP using the forward() method, which computes the activations for each layer in the network.
- Compute the error for the output layer by subtracting the targets from the activations of the output layer.
- Backpropagate the errors to compute the gradients for each layer in the network.
- Update the weights and biases for each layer in the network using the computed gradients.
"""